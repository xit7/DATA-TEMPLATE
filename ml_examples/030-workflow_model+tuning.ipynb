{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f763189",
   "metadata": {},
   "source": [
    "# Model selection + Hyperparameter tuning\n",
    "\n",
    "In practice typically combined with:\n",
    "* **clean train/validation/test** discipline (or nested CV - see below),\n",
    "* **pipelines** (preprocessing + model in one unit, cross-validated end-to-end),\n",
    "* **cross-validation** with multiple metrics,\n",
    "* a **final hold-out test** to report unbiased performance.\n",
    "\n",
    "## Outline (playbook)\n",
    "1. **Frame the problem & metric(s)** (eg. Classification? Use ROC AUC (ranking), F1 (imbalance), Accuracy (sanity), LogLoss (probability quality))\n",
    "2. **Split once → train / test** (keep test untouched until the very end).\n",
    "3. **Build a Pipeline** = preprocessing (e.g., imputation, scaling) + model.\n",
    "4. Define **candidate models + param grids**.\n",
    "5. **Cross-validated grid search per model**; collect CV metrics.\n",
    "6. **Compare models on CV**; pick the best (by primary metric).\n",
    "7. Refit winner on full train; evaluate on test; save artifacts.\n",
    "8. (Optional) Calibrate probabilities, threshold tuning for business tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8934f0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cross-validated Model Comparison (sorted by ROC AUC) ===\n",
      " Model                                                                                                             Best Params  CV ROC AUC  CV F1  CV Accuracy  CV LogLoss\n",
      "logreg                                                    {'model__C': 1, 'model__class_weight': None, 'model__penalty': 'l2'}      0.9959 0.9825       0.9780      0.0723\n",
      "    gb          {'model__learning_rate': 0.1, 'model__max_depth': 2, 'model__min_samples_leaf': 5, 'model__n_estimators': 200}      0.9948 0.9789       0.9736      0.1090\n",
      "    rf {'model__class_weight': 'balanced', 'model__max_depth': None, 'model__min_samples_leaf': 1, 'model__n_estimators': 200}      0.9924 0.9700       0.9626      0.1185\n",
      "\n",
      "Selected winner by CV ROC AUC: logreg\n",
      "\n",
      "=== Final Test Metrics (Winner) ===\n",
      "Test ROC AUC: 0.9954\n",
      "Test F1: 0.9861\n",
      "Test Accuracy: 0.9825\n",
      "Test LogLoss: 0.0777\n",
      "\n",
      "=== Classification Report (Winner @0.5 threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9762    0.9762    0.9762        42\n",
      "           1     0.9861    0.9861    0.9861        72\n",
      "\n",
      "    accuracy                         0.9825       114\n",
      "   macro avg     0.9812    0.9812    0.9812       114\n",
      "weighted avg     0.9825    0.9825    0.9825       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Model Selection + Hyperparameter Tuning (Classification)\n",
    "# ======================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, log_loss, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# --------------------------\n",
    "# 0) Data: use a clean built-in classification dataset\n",
    "# --------------------------\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Optional: inspect\n",
    "# print(X.head(), y.value_counts())\n",
    "\n",
    "# Train/test split (hold out test for honest final evaluation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 1) Preprocessing\n",
    "# --------------------------\n",
    "# This dataset is all numeric; we still show a ColumnTransformer for real-world readiness.\n",
    "num_features = X_train.columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 2) Candidate models and parameter grids\n",
    "# --------------------------\n",
    "models_and_grids = [\n",
    "    (\n",
    "        \"logreg\",\n",
    "        LogisticRegression(max_iter=1000, solver=\"lbfgs\"),\n",
    "        {\n",
    "            \"model__C\": [0.01, 0.1, 1, 10],\n",
    "            \"model__penalty\": [\"l2\"],\n",
    "            \"model__class_weight\": [None, \"balanced\"]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        \"rf\",\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [200, 500],\n",
    "            \"model__max_depth\": [None, 5, 10, 20],\n",
    "            \"model__min_samples_leaf\": [1, 2, 5],\n",
    "            \"model__class_weight\": [None, \"balanced\"]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        \"gb\",\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [200, 500],\n",
    "            \"model__learning_rate\": [0.05, 0.1],\n",
    "            \"model__max_depth\": [2, 3],\n",
    "            \"model__min_samples_leaf\": [1, 2, 5]\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# 3) Cross-validation setup & scoring\n",
    "# --------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Primary metric: ROC AUC; also collect others for context\n",
    "scoring = {\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"neg_log_loss\": \"neg_log_loss\"\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 4) Run GridSearchCV for each model\n",
    "# --------------------------\n",
    "comparison_rows = []\n",
    "best_estimators = {}\n",
    "\n",
    "for name, estimator, param_grid in models_and_grids:\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", estimator)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        refit=\"roc_auc\",            # refit the best params per model using ROC AUC\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Store the refit best estimator & metrics\n",
    "    best_estimators[name] = grid.best_estimator_\n",
    "\n",
    "    # Extract cross-validated means for our metrics\n",
    "    mean_roc_auc   = grid.cv_results_[\"mean_test_roc_auc\"][grid.best_index_]\n",
    "    mean_f1        = grid.cv_results_[\"mean_test_f1\"][grid.best_index_]\n",
    "    mean_acc       = grid.cv_results_[\"mean_test_accuracy\"][grid.best_index_]\n",
    "    mean_log_loss  = -grid.cv_results_[\"mean_test_neg_log_loss\"][grid.best_index_]  # flip sign\n",
    "\n",
    "    comparison_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": grid.best_params_,\n",
    "        \"CV ROC AUC\": round(mean_roc_auc, 4),\n",
    "        \"CV F1\": round(mean_f1, 4),\n",
    "        \"CV Accuracy\": round(mean_acc, 4),\n",
    "        \"CV LogLoss\": round(mean_log_loss, 4)\n",
    "    })\n",
    "\n",
    "# --------------------------\n",
    "# 5) Compare models by CV ROC AUC (primary)\n",
    "# --------------------------\n",
    "comparison_df = pd.DataFrame(comparison_rows).sort_values(by=\"CV ROC AUC\", ascending=False)\n",
    "print(\"\\n=== Cross-validated Model Comparison (sorted by ROC AUC) ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Pick the winner\n",
    "winner_name = comparison_df.iloc[0][\"Model\"]\n",
    "winner = best_estimators[winner_name]\n",
    "print(f\"\\nSelected winner by CV ROC AUC: {winner_name}\")\n",
    "\n",
    "# --------------------------\n",
    "# 6) Final evaluation on the untouched test set\n",
    "# --------------------------\n",
    "winner.fit(X_train, y_train)  # refit on full training data\n",
    "proba_test = winner.predict_proba(X_test)[:, 1]\n",
    "pred_test  = (proba_test >= 0.5).astype(int)  # simple 0.5 threshold; tune if needed\n",
    "\n",
    "final_metrics = {\n",
    "    \"Test ROC AUC\": roc_auc_score(y_test, proba_test),\n",
    "    \"Test F1\": f1_score(y_test, pred_test),\n",
    "    \"Test Accuracy\": accuracy_score(y_test, pred_test),\n",
    "    \"Test LogLoss\": log_loss(y_test, proba_test)\n",
    "}\n",
    "print(\"\\n=== Final Test Metrics (Winner) ===\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n=== Classification Report (Winner @0.5 threshold) ===\")\n",
    "print(classification_report(y_test, pred_test, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4efe1a",
   "metadata": {},
   "source": [
    "# A note about the order and avoiding data leakage between train & test\n",
    "Test set must match the structure of the training data (e.g., same scaling, same encoding). But if you fit your imputer/scaler/encoder *before splitting*, you’ve already used information from the test set during preprocessing. **That’s leakage**.\n",
    "\n",
    "**Example**:\n",
    "* Suppose you scale features with StandardScaler. If you compute the mean and std on the whole dataset before splitting, the scaler has already seen the test distribution.\n",
    "* Then your test evaluation is no longer a true “unseen” evaluation.\n",
    "\n",
    "**Therefor use the Correct Order:**\n",
    "1. Split once into train and test.\n",
    "   * The test set is put aside and not touched until the end.\n",
    "2.\tBuild a pipeline that contains:\n",
    "   * Imputation, scaling, encoding, feature selection, etc.\n",
    "   * Followed by the model.\n",
    "3.\tFit the pipeline only on training data.\n",
    "   * The scaler, imputer, encoder all “learn” their parameters (e.g., mean, std, category mapping) only from the training set.\n",
    "4.\tApply the trained pipeline to the test set.\n",
    "   * The pipeline reuses the learned transformations on the test data.\n",
    "\n",
    "\n",
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Split FIRST\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build preprocessing\n",
    "num_features = X_train.columns\n",
    "num_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_transformer, num_features)\n",
    "])\n",
    "\n",
    "# Build pipeline = preprocessing + model\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit on train only\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test\n",
    "test_score = pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737941f8",
   "metadata": {},
   "source": [
    "# The Problem with “Normal” Cross-Validation + GridSearch\n",
    "\n",
    "If you want the most rigorous selection, wrap this in nested CV (outer CV for selection, inner CV for tuning).\n",
    "\n",
    "**When you run GridSearchCV (or RandomizedSearchCV):**\n",
    "* You split data into folds.\n",
    "* For each candidate hyperparameter set, you train on (k–1) folds and validate on the held-out fold.\n",
    "* You pick the hyperparameters with the best mean CV score.\n",
    "\n",
    "⚠️ **But here’s the catch:**\n",
    "* Because you use the same CV both for hyperparameter tuning and for estimating performance, your CV score is a slightly optimistic estimate (it has “peeked” at validation).\n",
    "\n",
    "## The Nested CV Solution\n",
    "\n",
    "**Nested CV introduces two layers of cross-validation**:\n",
    "1. **Outer loop (evaluation loop)**:\n",
    "    * Split the dataset into outer folds.\n",
    "    * Each outer fold acts as a test set once.\n",
    "2. **Inner loop (tuning loop)**:\n",
    "  * Inside each training split from the outer loop, run GridSearchCV/RandomizedSearchCV to select the best hyperparameters.\n",
    "  * Refit the best model on that inner training data.\n",
    "3. **Evaluate**:\n",
    "* Test the tuned model on the outer test fold.\n",
    "* Collect the scores across all outer folds → unbiased estimate of true performance.\n",
    "\n",
    "**Why It Matters**\n",
    "* Normal CV = good enough when you just want to pick a model.\n",
    "* Nested CV = necessary if you want an unbiased estimate of generalization error after tuning.\n",
    "\n",
    "That’s why in research papers and benchmarking, nested CV is the standard.\n",
    "\n",
    "**ℹ️ In business practice, many teams skip nested CV because it’s expensive, and instead:**\n",
    "* Keep a hold-out test set (like I showed earlier), or\n",
    "* Use cross-validation for tuning, then evaluate once on the untouched test set.\n",
    "\n",
    "### Simple Example Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67d95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV ROC AUC scores: [0.99836227 0.99705208 0.98511905 0.99966931 0.99865862]\n",
      "Mean performance (unbiased): 0.9957722649419342\n",
      "Std: 0.005391754587558533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# (Optional) silence only LR convergence warnings during CV\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Pipeline: scale -> logistic regression\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=5000))  # higher ceiling helps convergence\n",
    "])\n",
    "\n",
    "# Hyperparameter grid (legal combos only)\n",
    "param_grid = [\n",
    "    {   # lbfgs (fast, stable for L2)\n",
    "        \"lr__solver\": [\"lbfgs\"],\n",
    "        \"lr__penalty\": [\"l2\"],\n",
    "        \"lr__C\": np.logspace(-3, 3, 7),   # 0.001 ... 1000\n",
    "    },\n",
    "    {   # liblinear (good for small/binary; supports L2 here to keep parity)\n",
    "        \"lr__solver\": [\"liblinear\"],\n",
    "        \"lr__penalty\": [\"l2\"],\n",
    "        \"lr__C\": np.logspace(-3, 3, 7),\n",
    "    },\n",
    "    {   # saga (scalable; also supports l1/elasticnet if you want to extend)\n",
    "        \"lr__solver\": [\"saga\"],\n",
    "        \"lr__penalty\": [\"l2\"],\n",
    "        \"lr__C\": np.logspace(-3, 3, 7),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Inner loop (tuning)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=inner_cv,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# Outer loop (unbiased evaluation)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nested_scores = cross_val_score(grid, X, y, cv=outer_cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "print(\"Nested CV ROC AUC scores:\", nested_scores)\n",
    "print(\"Mean performance (unbiased):\", np.mean(nested_scores))\n",
    "print(\"Std:\", np.std(nested_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
