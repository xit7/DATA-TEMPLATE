{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03737c",
   "metadata": {},
   "source": [
    "# KNN Classifier using features that matter\n",
    "\n",
    "**Goal**: Build a KNN classifier that uses only the features that matter, and gives more weight to the most useful ones.\n",
    "**Dataset**: sklearn.datasets.load_breast_cancer (binary classification; 30 numeric features).\n",
    "\n",
    "## Plan (broad → focused):\n",
    "1. **Baseline with all features** (Pipeline: StandardScaler → KNeighborsClassifier).\n",
    "Purpose: establish a reference accuracy.\n",
    "2. **Filter method**: rank features by Mutual Information (MI) with the target, then test top-k subsets via cross-validation.\n",
    "Purpose: fast, model-agnostic signal of relevance.\n",
    "3. **Wrapper method**: Sequential Forward Selection (SFS) wrapped around the KNN pipeline.\n",
    "Purpose: pick a subset that optimizes CV accuracy for the actual model.\n",
    "4. **Model-agnostic importance**: Permutation importance on a validation set using the chosen subset.\n",
    "Purpose: sanity-check and quantify which selected features the model truly uses.\n",
    "5. **Weighted KNN**: convert permutation importances into per-feature weights and use weighted Minkowski distance (metric=\"wminkowski\") so important features count more.\n",
    "Purpose: bake “feature importance” directly into the distance metric.\n",
    "6. **Finalization**: pick (unweighted vs weighted) by validation accuracy, refit on train+val, and evaluate on the test set.\n",
    "Why this order? We start broad to understand signals cheaply (filter), then let the model choose (wrapper), then verify (permutation), then use that knowledge (weights), and finally lock it in (test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b1a4e",
   "metadata": {},
   "source": [
    "# Data split (60/20/20, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a19408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> train: (341, 30) val: (114, 30) test: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# KNN classification with feature understanding, selection, and weighting\n",
    "# --------------------------------------------------------------\n",
    "# Requires: scikit-learn, numpy, pandas, matplotlib\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Utilities -------------------------------------------------\n",
    "\n",
    "def save_or_show(fig, filename=None):\n",
    "    \"\"\"\n",
    "    Saves the figure to /mnt/data/knn_feature_selection_figs (if filename given)\n",
    "    and tries to show it as well. If show() fails (e.g., headless), the file is still saved.\n",
    "    \"\"\"\n",
    "    if filename is not None:\n",
    "        out_dir = os.path.join(os.getcwd(), \"knn_feature_selection_figs\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        path = os.path.join(out_dir, filename)\n",
    "        fig.savefig(path, bbox_inches=\"tight\")\n",
    "        print(f\"[saved] {path}\")\n",
    "    try:\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close(fig)\n",
    "\n",
    "def cv_score_with_features(X, y, feature_list, n_neighbors=5, cv=None):\n",
    "    \"\"\"Cross-validated accuracy using a standard KNN pipeline on a given feature subset.\"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=n_neighbors))\n",
    "    ])\n",
    "    scores = cross_val_score(pipe, X[feature_list], y, cv=cv, scoring=\"accuracy\")\n",
    "    return scores.mean(), scores\n",
    "\n",
    "# --- 1) Load & split ------------------------------------------\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_full = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y_full = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "# 60% train, 20% val, 20% test (stratified)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Shapes -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84d86e",
   "metadata": {},
   "source": [
    "## Baseline (all features → StandardScaler → KNN(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd5b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (all features) CV accuracy: mean=0.9648, scores=[0.971  0.9118 1.     0.9706 0.9706]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2) Baseline with all features -----------------------------\n",
    "\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "baseline_cv = cross_val_score(baseline_pipe, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "print(f\"\\nBaseline (all features) CV accuracy: mean={baseline_cv.mean():.4f}, scores={np.round(baseline_cv, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7d0ac",
   "metadata": {},
   "source": [
    "## Filter: Mutual Information (MI) ranking + top-k evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f17981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) FILTER: Mutual Information ranking --------------------\n",
    "\n",
    "# MI is neighbor-based; MinMax scaling helps it compare distances fairly\n",
    "mm = MinMaxScaler()\n",
    "X_train_mm = mm.fit_transform(X_train)\n",
    "mi = mutual_info_classif(X_train_mm, y_train, random_state=RANDOM_STATE)\n",
    "mi_series = pd.Series(mi, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by Mutual Information:\")\n",
    "print(mi_series.head(10))\n",
    "\n",
    "# Plot top-15 MI features\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "mi_series.iloc[:15][::-1].plot(kind=\"barh\")\n",
    "plt.title(\"Mutual Information — Top 15 Features\")\n",
    "plt.xlabel(\"MI score\")\n",
    "plt.legend([\"Feature MI scores\"])\n",
    "save_or_show(fig)\n",
    "\n",
    "# Evaluate top-k by MI (1..15)\n",
    "k_grid = list(range(1, 16))\n",
    "mi_cv_means = []\n",
    "for k in k_grid:\n",
    "    feats = list(mi_series.index[:k])\n",
    "    mean_acc, _ = cv_score_with_features(X_train, y_train, feats, n_neighbors=5, cv=cv)\n",
    "    mi_cv_means.append(mean_acc)\n",
    "\n",
    "best_k_mi = k_grid[int(np.argmax(mi_cv_means))]\n",
    "best_mi_feats = list(mi_series.index[:best_k_mi])\n",
    "print(f\"\\nMI selection → best k={best_k_mi} (CV acc={max(mi_cv_means):.4f})\")\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.plot(k_grid, mi_cv_means, marker=\"o\")\n",
    "plt.title(\"Filter (MI): CV accuracy vs #features\")\n",
    "plt.xlabel(\"# top-MI features\")\n",
    "plt.ylabel(\"CV accuracy\")\n",
    "plt.legend([\"Top-MI selection\"])\n",
    "plt.grid(True)\n",
    "save_or_show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b848b",
   "metadata": {},
   "source": [
    "## Wrapper: Sequential Forward Selection (SFS) around the KNN pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) WRAPPER: Sequential Forward Selection (SFS) -----------\n",
    "\n",
    "# Use a pipeline inside SFS so scaling happens correctly for each subset\n",
    "sfs_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "sfs_cv_means = []\n",
    "sfs_feature_sets = []\n",
    "for k in k_grid:\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        sfs_base,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"accuracy\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_train, y_train)\n",
    "    mask = sfs.get_support()\n",
    "    feats_k = list(X_train.columns[mask])\n",
    "    sfs_feature_sets.append(feats_k)\n",
    "    mean_acc, _ = cv_score_with_features(X_train, y_train, feats_k, n_neighbors=5, cv=cv)\n",
    "    sfs_cv_means.append(mean_acc)\n",
    "\n",
    "best_k_sfs = k_grid[int(np.argmax(sfs_cv_means))]\n",
    "best_sfs_feats = sfs_feature_sets[int(np.argmax(sfs_cv_means))]\n",
    "print(f\"\\nSFS selection → best k={best_k_sfs} (CV acc={max(sfs_cv_means):.4f})\")\n",
    "print(\"Top features (first 8 shown):\", best_sfs_feats[:8], \"...\")\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.plot(k_grid, sfs_cv_means, marker=\"o\")\n",
    "plt.title(\"Wrapper (SFS): CV accuracy vs #features\")\n",
    "plt.xlabel(\"# SFS-selected features\")\n",
    "plt.ylabel(\"CV accuracy\")\n",
    "plt.legend([\"SFS selection\"])\n",
    "plt.grid(True)\n",
    "save_or_show(fig)\n",
    "\n",
    "# We'll proceed with the SFS-selected subset (usually strongest for KNN)\n",
    "chosen_feats = best_sfs_feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8661ce",
   "metadata": {},
   "source": [
    "## Model-agnostic check: Permutation importance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) Permutation importance on validation set --------------\n",
    "\n",
    "sfs_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "sfs_pipe.fit(X_train[chosen_feats], y_train)\n",
    "val_pred = sfs_pipe.predict(X_val[chosen_feats])\n",
    "val_acc = accuracy_score(y_val, val_pred)\n",
    "print(f\"\\nValidation accuracy (SFS subset, unweighted distance): {val_acc:.4f}\")\n",
    "\n",
    "perm = permutation_importance(\n",
    "    sfs_pipe, X_val[chosen_feats], y_val,\n",
    "    n_repeats=30, random_state=RANDOM_STATE, scoring=\"accuracy\"\n",
    ")\n",
    "perm_series = pd.Series(perm.importances_mean, index=chosen_feats).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPermutation importance (top 10):\")\n",
    "print(np.round(perm_series.head(10), 5))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "perm_series.iloc[:min(15, len(chosen_feats))][::-1].plot(kind=\"barh\")\n",
    "plt.title(\"Permutation Importance on Validation\")\n",
    "plt.xlabel(\"Mean accuracy decrease when permuted\")\n",
    "plt.legend([\"Permutation importance\"])\n",
    "save_or_show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243a881",
   "metadata": {},
   "source": [
    "## Weighted KNN (turn validation-set importances into distance weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6) Weighted KNN via feature scaling instead of 'wminkowski' ---\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureWeightScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Multiplies each feature by w**(1/p), which makes standard Minkowski(p)\n",
    "    equivalent to Weighted-Minkowski with weights w.\n",
    "    \"\"\"\n",
    "    def __init__(self, w, p=2):\n",
    "        self.w = np.asarray(w, dtype=float)\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X)\n",
    "        if X.shape[1] != self.w.shape[0]:\n",
    "            raise ValueError(f\"Weight length {self.w.shape[0]} != n_features {X.shape[1]}\")\n",
    "        # zero weights are allowed (feature gets ignored)\n",
    "        self.scale_ = np.power(np.clip(self.w, a_min=0.0, a_max=None), 1.0 / self.p)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.asarray(X) * self.scale_\n",
    "\n",
    "# (same as before) turn permutation importance into weights, normalize to mean=1\n",
    "imp = perm_series.reindex(chosen_feats).fillna(0.0).clip(lower=0.0)\n",
    "if (imp == 0).all():\n",
    "    w = np.ones(len(chosen_feats))\n",
    "else:\n",
    "    w = imp.values\n",
    "    w = w / (w.mean() if w.mean() != 0 else 1.0)\n",
    "\n",
    "p = 2  # Minkowski p (2 = Euclidean)\n",
    "weighted_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"wscale\", FeatureWeightScaler(w=w, p=p)),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=p))\n",
    "])\n",
    "\n",
    "weighted_pipe.fit(X_train[chosen_feats], y_train)\n",
    "val_pred_w = weighted_pipe.predict(X_val[chosen_feats])\n",
    "val_acc_w = accuracy_score(y_val, val_pred_w)\n",
    "print(f\"Validation accuracy (SFS subset, weighted distance via scaling): {val_acc_w:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43fbdb",
   "metadata": {},
   "source": [
    "## Finalization and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7) Final model & test evaluation (unchanged logic) -------\n",
    "\n",
    "use_weighted = val_acc_w >= val_acc\n",
    "final_pipe = weighted_pipe if use_weighted else sfs_pipe\n",
    "final_label = \"Weighted KNN (scaled)\" if use_weighted else \"Unweighted KNN\"\n",
    "\n",
    "X_train_final = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_final = pd.concat([y_train, y_val], axis=0)\n",
    "final_pipe.fit(X_train_final[chosen_feats], y_train_final)\n",
    "\n",
    "test_pred = final_pipe.predict(X_test[chosen_feats])\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(f\"\\nFINAL MODEL: {final_label} using {len(chosen_feats)} features\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nConfusion matrix (test):\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, test_pred, target_names=data.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
