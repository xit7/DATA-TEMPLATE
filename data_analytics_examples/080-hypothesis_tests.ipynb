{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09ab14a",
   "metadata": {},
   "source": [
    "# Hypothesis Tests (or significance tests)\n",
    "> Hypothesis tests are ubiquitous in statistics. They are used to determine whether there is enough evidence in a sample of data to infer that a certain condition is true for the entire population. Statistical hypothesis testing was invented a way ro protect researchers from being fooled by random chance.\n",
    "\n",
    "## The Null Hypothesis\n",
    "> This is the baseline assumption that the treatments are equivalent, and any differences observed are due to random chance. This baseline assumption is called the **null hypothesis**, denoted $H_0$.\n",
    "> The hope then is, that we can in fact prove the null hypothesis *wrong*, and show that the outcomes for Group A and Be are more different than what chance might produce.\n",
    "\n",
    "## Permutation (Resampling) Test ‚Äî Intuitive Explanation\n",
    "In a permutation test, we simulate what would happen if the null hypothesis were true ‚Äî that is, if both groups (A and B) actually came from the same distribution.\n",
    "\n",
    "Imagine we take all the results from Group A and Group B and toss them into one big basket.\n",
    "If the null hypothesis is true ‚Äî meaning the treatment makes no real difference ‚Äî then which result came from A or B shouldn‚Äôt matter at all.\n",
    "\n",
    "So, to test this assumption, we:\n",
    "\n",
    "1. Shuffle the results randomly between two new groups of the same sizes as A and B.\n",
    "2. Measure the difference between these new groups (e.g., in average conversion rate).\n",
    "3. Repeat this shuffle thousands of times to see what kinds of differences we‚Äôd expect just by chance.\n",
    "4. Finally, we compare our real observed difference to this ‚Äúwhat-if‚Äù world of random chance.\n",
    "\n",
    "If our actual difference is far greater than almost all the shuffled ones, it‚Äôs unlikely to be random ‚Äî giving us evidence that the groups truly differ, and the null hypothesis can be rejected.\n",
    "\n",
    "Technically, the **p-value** is the proportion of shuffled results that are as extreme or more extreme than your observed difference.\n",
    "If this proportion (p-value) is very small (e.g., < 0.05), it means the observed difference is unlikely to occur under random chance ‚Äî providing evidence against $ùêª_0$.\n",
    "\n",
    "### Storytelling Analogy\n",
    "* Think of your A/B test results as a deck of cards ‚Äî some marked ‚ÄúA‚Äù and some ‚ÄúB,‚Äù each showing how well that user performed.\n",
    "* If the null hypothesis is true, the ‚ÄúA‚Äù and ‚ÄúB‚Äù labels don‚Äôt really matter ‚Äî they‚Äôre just stickers on the cards. So, you shuffle the deck, deal it back into two new piles, and measure the difference again.\n",
    "* You repeat this shuffle many times, building up an idea of how big a difference you‚Äôd expect just by chance if A and B were really the same.\n",
    "* Now, if your real observed difference is much bigger than what usually happens in your shuffles, it‚Äôs like drawing a hand that‚Äôs astronomically rare ‚Äî you‚Äôve got good evidence that the treatments aren‚Äôt equivalent and that the effect is real, not random."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
